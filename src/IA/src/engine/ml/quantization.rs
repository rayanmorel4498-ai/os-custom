// Model Quantization - Int8, FP16, Dynamic quantization

use alloc::sync::Arc;
use spin::Mutex;
use crate::prelude::Vec;
use crate::utils::debug_writer::DebugWriter;

#[derive(Clone, Debug)]
pub enum QuantizationStrategy {
    Int8,
    Int16,
    FP16,
    Dynamic,
}

/// Model Quantizer - R√©duit la taille et am√©liore la vitesse
pub struct ModelQuantizer {
    strategy: QuantizationStrategy,
    scale_factors: Arc<Mutex<Vec<f64>>>,
    zero_points: Arc<Mutex<Vec<i32>>>,
    compression_ratio: Arc<Mutex<f64>>,
}

impl ModelQuantizer {
    pub fn new(strategy: QuantizationStrategy) -> Self {
        let msg = alloc::format!("[Quantizer] Strategy: {:?}", strategy);
        DebugWriter::info(&msg);
        ModelQuantizer {
            strategy,
            scale_factors: Arc::new(Mutex::new(Vec::new())),
            zero_points: Arc::new(Mutex::new(Vec::new())),
            compression_ratio: Arc::new(Mutex::new(1.0)),
        }
    }

    /// Quantifier un mod√®le
    pub async fn quantize(&self, model_size_mb: f64) -> f64 {
        let compression = match self.strategy {
            QuantizationStrategy::Int8 => 0.25,   // 75% compression
            QuantizationStrategy::Int16 => 0.5,   // 50% compression
            QuantizationStrategy::FP16 => 0.5,    // 50% compression
            QuantizationStrategy::Dynamic => 0.3, // 70% compression
        };

        let new_size = model_size_mb * compression;
        *self.compression_ratio.lock() = compression;

        DebugWriter::info(&format!(
            "üîß Quantization ({:?}): {:.1}MB ‚Üí {:.1}MB (compression: {:.0}%)",
            self.strategy,
            model_size_mb,
            new_size,
            (1.0 - compression) * 100.0
        ));

        new_size
    }

    /// Post-training quantization
    pub async fn post_training_quantize(&self, weights: &[f64]) -> Vec<i8> {
        let min_val = weights.iter().copied().fold(f64::INFINITY, f64::min);
        let max_val = weights.iter().copied().fold(f64::NEG_INFINITY, f64::max);

        let scale = (max_val - min_val) / 255.0;
        let zero_point = (-min_val / scale) as i32;

        *self.scale_factors.lock() = vec![scale];
        *self.zero_points.lock() = vec![zero_point];

        // Convertir en int8
        let quantized: Vec<i8> = weights
            .iter()
            .map(|&w| (((w - min_val) / scale) as i32 + zero_point).min(127).max(-128) as i8)
            .collect();

        DebugWriter::info(&format!("‚úì PTQ complete: {} weights ‚Üí {} int8", weights.len(), quantized.len()));
        quantized
    }

    /// Quantization aware training simulation
    pub async fn qat_simulate(&self, iterations: u32) -> f64 {
        DebugWriter::info(&format!("üìä QAT (Quantization Aware Training) - {} iterations", iterations));

        let mut accuracy = 0.95;
        for i in 0..iterations {
            // Simulation: slight accuracy drop with quantization
            accuracy -= 0.001;
            
            if i % 10 == 0 {
                DebugWriter::info(&format!("  Iteration {}/{}: Accuracy = {:.2}%", i, iterations, accuracy * 100.0));
            }
        }

        DebugWriter::info(&format!("‚úì QAT completed: Final accuracy = {:.2}%", accuracy * 100.0));
        accuracy
    }

    pub async fn get_compression_ratio(&self) -> f64 {
        *self.compression_ratio.lock()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::utils::test_runtime::block_on;

    #[test]
    fn test_int8_quantization() {
        block_on(async {
        let quantizer = ModelQuantizer::new(QuantizationStrategy::Int8);
        let size = quantizer.quantize(100.0);
        assert!(size < 30.0); // Should be < 25MB after compression
        });
    }

    #[test]
    fn test_post_training_quantize() {
        block_on(async {
        let quantizer = ModelQuantizer::new(QuantizationStrategy::Int8);
        let weights = vec![0.1, 0.5, 0.9, 0.3];
        let quantized = quantizer.post_training_quantize(&weights);
        assert_eq!(quantized.len(), 4);
        });
    }
}
